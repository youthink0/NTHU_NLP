{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJBKoaDrpWD-"
   },
   "source": [
    "# Week 06: Collocation Extraction\n",
    "In Assignment 5, we found all skip-grams and their frequencies in <u>*wiki1G.txt*</u>. This week, we want to use the result of assignment 5 to extract collocations of [AKL verbs](https://uclouvain.be/en/research-institutes/ilc/cecl/academic-keyword-list.html). We will use [Smadja‚Äôs algorithm](https://aclanthology.org/J93-1007.pdf) to do it. Here are some basic terms need to be explain. \n",
    "\n",
    "We take \"*dpend*\" as an example:\n",
    "\n",
    "<img src=\"https://imgur.com/cPyd7Gr.jpg\" >\n",
    "\n",
    "In this case, we want to find the collocations of \"depend\". Then, \"depend\" is called **base word** and marked as $W$. As for \"on\", \"the\", \"for\"..., they are called **collocate** and marked as $W_{i}$ where **i** represents their serial number. $P_{j}$ means the frequency of $W$ and $W_{i}$ with distance j. And **Freq** is the sum of frequencies of all distances.\n",
    "\n",
    "There are three conditions to filter the skipgram to find collocations. We will go through three conditions below.\n",
    "\n",
    "Considering that some students did not complete Assignment 5, in order to avoid them being unable to do assignment 6, we provide you with a file of calculated skipgram with frequencies, called **AKL_skipgram.tsv**. It only keeps the skipgrams with any AKL verb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TBxDYAFPpWEA"
   },
   "source": [
    "## Read Data\n",
    "<font color=\"red\">**[ TODO ]**</font> Please read <u>*AKL_skipgram.tsv*</u> and store it in the way you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "sVGZSm9fpWEB"
   },
   "outputs": [],
   "source": [
    "#### here are some hyperparameter\n",
    "k0 = 1\n",
    "k1 = 1\n",
    "U0 = 10\n",
    "base_word = \"depend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "ekEseC_PpWEB"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "## read file here\n",
    "skipgram_col = ['W', 'Wi', 'Freq', 'P-5', 'P-4', 'P-3', 'P-2', 'P-1', 'P1', 'P2', 'P3', 'P4', 'P5']\n",
    "with open(os.path.join('data', 'AKL_skipgram.tsv'), encoding=\"utf-8\") as f:\n",
    "    tmp_df = pd.read_csv(f, sep='\\t') #store tsv as DataFrame, seperate by \"\\t\"\n",
    "    tmp_df.columns = skipgram_col #give DataFrame column names     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    17\n",
      "1     6\n",
      "2     0\n",
      "dtype: int64\n",
      "         W  P-5\n",
      "0        0    3\n",
      "1        0    4\n",
      "2        0    0\n",
      "3        0    4\n",
      "4        0    0\n",
      "...     ..  ...\n",
      "5542979  ùïè    1\n",
      "5542980  ùõø    0\n",
      "5542981  ùõø    0\n",
      "5542982  ¢íâ    0\n",
      "5542983  ¢íâ    0\n",
      "\n",
      "[5542984 rows x 2 columns]\n",
      "   W       Wi  Freq  P-5  P-4  P-3  P-2  P-1  P1  P2  P3  P4  P5\n",
      "0  0  account    29    3    4   14    7    0   0   0   0   0   1\n",
      "1  0  achieve    13    4    3    2    0    4   0   0   0   0   0\n",
      "2  0  acquire     2    0    0    0    0    0   0   0   1   0   1\n",
      "              W   Wi  Freq  P-5  P-4  P-3  P-2  P-1  P1  P2  P3  P4  P5\n",
      "1600255  depend    0     1    1    0    0    0    0   0   0   0   0   0\n",
      "1600256  depend  000     2    0    2    0    0    0   0   0   0   0   0\n",
      "1600257  depend  035     1    1    0    0    0    0   0   0   0   0   0\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "print((tmp_df['P-5'] + tmp_df['P-3'])[:3])\n",
    "\n",
    "print( tmp_df[ ['W', 'P-5'] ] )\n",
    "\n",
    "print(tmp_df.head(3))\n",
    "\n",
    "print(tmp_df[ tmp_df['W'] == \"depend\"].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3N9bVOqOpWEB"
   },
   "source": [
    "## C1 Condition\n",
    "C1 helps eliminate the collocates that are not frequent enough. This condition specifies that the frequency of appearance of $W_{i}$ in the neighborhood of $W$ must be at least one standard deviation above the average.\n",
    "\n",
    "The formula is here:\n",
    "\n",
    "$$strength = \\frac{freq - \\bar{f}}{\\sigma} \\geq k_{0} = 1$$\n",
    "\n",
    "where $freq$ is the frequency of certain collocate, (e.g., 2573 for \"on\") and \n",
    "\n",
    "$\\bar{f}$ is the average frequencies of all collocates and \n",
    "\n",
    "${\\sigma}$ is the standard deviation of frequencies of all collocates.\n",
    "\n",
    "<font color=\"red\">**[ TODO ]**</font> Please follow the condition to filter the skipgrams of \"depend\" and keep some which pass the condition.\n",
    "\n",
    "The ouput sholud have `collocate` with its `strength`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "id": "nQR2AYDxpWEC"
   },
   "outputs": [],
   "source": [
    "def C1_filter(base_word, filter_word):\n",
    "    ### [TODO]\n",
    "    tmp = base_word[ base_word['W'] == filter_word ] #ÂÖàÁØ©ÈÅ∏Âá∫ÊØèÂÄãword\n",
    "    sum_tmp = tmp['Freq'].sum() #freqÁ∏ΩÂÄº\n",
    "    cnt_tmp = tmp.shape[0] #ÂàóÊï∏\n",
    "    avg_tmp = sum_tmp/cnt_tmp #f bar\n",
    "    std_tmp = np.std(tmp['Freq'], ddof=0) #Ê®ôÊ∫ñÂ∑Æ\n",
    "    tmp_store = tmp[ (tmp['Freq'] - avg_tmp)/std_tmp >= k0 ] #ÁØ©ÈÅ∏strength\n",
    "    tmp_store['strength'] = ((tmp['Freq'] - avg_tmp)/std_tmp).round(3) #Êñ∞Â¢ûcolumn\n",
    "    '''\n",
    "    a = base_word['W'].unique()\n",
    "    for i in a[:10]:\n",
    "        tmp = base_word[ base_word['W'] == i ] #ÂÖàÁØ©ÈÅ∏Âá∫ÊØèÂÄãword\n",
    "        sum_tmp = tmp['Freq'].sum() #freqÁ∏ΩÂÄº\n",
    "        cnt_tmp = tmp.shape[0] #ÂàóÊï∏\n",
    "        avg_tmp = sum_tmp/cnt_tmp\n",
    "        std_tmp = np.std(tmp['Freq'], ddof=1)\n",
    "        tmp_store = tmp[ (tmp['Freq'] - avg_tmp)/std_tmp >= k0 ]\n",
    "        #print(i, sum_tmp, cnt_tmp, std_tmp)\n",
    "    '''   \n",
    "    return tmp_store[ ['Wi', 'strength'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "id": "ikso3UZIpWEC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Wi  strength\n",
      "1600339           a     6.381\n",
      "1600497         all     1.151\n",
      "1600518        also     1.133\n",
      "1600550          an     1.367\n",
      "1600558         and    15.183\n",
      "1600640         are     1.962\n",
      "1600675          as     2.395\n",
      "1600957         but     1.529\n",
      "1600961          by     1.042\n",
      "1600985         can     1.421\n",
      "1601752          do     1.656\n",
      "1601758        does     5.299\n",
      "1602246         for     4.686\n",
      "1602269     formula     1.565\n",
      "1602694          in     5.876\n",
      "1602887          is     2.611\n",
      "1602896          it     2.287\n",
      "1602901         its     1.818\n",
      "1603255         may     2.864\n",
      "1603569         not     8.437\n",
      "1603628          of    23.461\n",
      "1603648          on    46.313\n",
      "1603654        only     1.295\n",
      "1603678          or     2.485\n",
      "1603712       other     1.656\n",
      "1604155  properties     1.042\n",
      "1604541           s     2.161\n",
      "1604817        some     1.187\n",
      "1605014        such     1.439\n",
      "1605166        that     7.247\n",
      "1605168         the    44.707\n",
      "1605169       their     2.828\n",
      "1605193       these     1.944\n",
      "1605194        they     2.233\n",
      "1605200        this     1.908\n",
      "1605231          to     8.419\n",
      "1605336        type     1.295\n",
      "1605408        upon     4.902\n",
      "1605596       which     4.379\n",
      "1605620        will     3.784\n",
      "1605661       would     1.601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a0985\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "filtered_by_C1 = C1_filter(tmp_df, 'depend')\n",
    "### Print\n",
    "print(filtered_by_C1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYguZMfwpWED"
   },
   "source": [
    "<font color=\"green\">Expected output: </font> (The order isn't important.)\n",
    "\n",
    "> a {'strength': 6.381}   \n",
    "> all {'strength': 1.151}   \n",
    "> also {'strength': 1.133}   \n",
    "> an {'strength': 1.367}   \n",
    "> and {'strength': 15.183}   \n",
    "> are {'strength': 1.962}   \n",
    "> as {'strength': 2.395}   \n",
    "> but {'strength': 1.529}   \n",
    "> by {'strength': 1.042}   \n",
    "> can {'strength': 1.421}   \n",
    "> do {'strength': 1.656}   \n",
    "> does {'strength': 5.299}   \n",
    "> for {'strength': 4.686}   \n",
    "> formula {'strength': 1.565}   \n",
    "> in {'strength': 5.876}   \n",
    "> is {'strength': 2.611}   \n",
    "> it {'strength': 2.287}   \n",
    "> its {'strength': 1.818}   \n",
    "> may {'strength': 2.864}   \n",
    "> not {'strength': 8.437}   \n",
    "> of {'strength': 23.461}   \n",
    "> on {'strength': 46.313}   \n",
    "> only {'strength': 1.295}   \n",
    "> or {'strength': 2.485}   \n",
    "> other {'strength': 1.656}   \n",
    "> properties {'strength': 1.042}   \n",
    "> s {'strength': 2.161}   \n",
    "> some {'strength': 1.187}   \n",
    "> such {'strength': 1.439}   \n",
    "> that {'strength': 7.247}   \n",
    "> the {'strength': 44.707}   \n",
    "> their {'strength': 2.828}   \n",
    "> these {'strength': 1.944}   \n",
    "> they {'strength': 2.233}   \n",
    "> this {'strength': 1.908}   \n",
    "> to {'strength': 8.419}   \n",
    "> type {'strength': 1.295}   \n",
    "> upon {'strength': 4.902}   \n",
    "> which {'strength': 4.379}   \n",
    "> will {'strength': 3.784}   \n",
    "> would {'strength': 1.601}   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4lWRmQrGpWED"
   },
   "source": [
    "## C2 Condition\n",
    "C2 requires that the histogram of the 10 relative frequencies of appearance of $W_i$ within five words of $W$ (or $p^j_i$s) have at least one spike. If the histogram is flat, it will be rejected by this condition.\n",
    "\n",
    "The formula is here:\n",
    "\n",
    "$$spread = \\frac{\\Sigma^{10}_{j=1}(p^j_i - \\bar{p_i})^2}{10} \\geq U_{0} = 10$$\n",
    "\n",
    "where $p^j_i$ is the frequency of certain collocate with a distance of *j*, (e.g., 16 for \"on\" when its distance is -5) and \n",
    "\n",
    "$\\bar{p_i}$ is the average frequencies of \"on\" with any distance \n",
    "\n",
    "<font color=\"red\">**[ TODO ]**</font> Please follow C2 to filter the result of C1 and keep some which pass C2.\n",
    "\n",
    "The ouput sholud have `collocate` with `strength` and `spread`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "id": "s2p0LbZtpWEE"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def C2_filter(base_word, filtered_by_C1, filter_word):\n",
    "    ### [TODO]\n",
    "    bs = base_word[ base_word['W'] == filter_word ] #ÁØ©ÈÅ∏Âá∫depend \n",
    "    bs = bs[ bs['Wi'].isin(filtered_by_C1['Wi']) ] #ÁØ©ÈÅ∏Âá∫filterÁöÑWi\n",
    "    bs['Pi_bar'] = (bs['P-5']+bs['P-4']+bs['P-3']+bs['P-2']+bs['P-1']+bs['P1']+bs['P2']+bs['P3']+bs['P4']+bs['P5'])/10\n",
    "    bs['spread'] = 0\n",
    "    for i in range(10): #Â•óÂÖ¨Âºè\n",
    "        bs['spread'] += np.square( bs[bs.columns[i+3]] - bs['Pi_bar'])\n",
    "    bs = bs[ bs['spread']/10 >= U0 ] #ÁØ©ÈÅ∏spread\n",
    "    filtered_by_C1['spread'] = bs['spread']/10 #Êñ∞Â¢ûspread column\n",
    "    return filtered_by_C1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "id": "OJPDVL0WpWEE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Wi  strength     spread\n",
      "1600339           a     6.381     777.29\n",
      "1600497         all     1.151      29.89\n",
      "1600518        also     1.133     208.96\n",
      "1600550          an     1.367      56.29\n",
      "1600558         and    15.183    2170.41\n",
      "1600640         are     1.962      98.84\n",
      "1600675          as     2.395     104.96\n",
      "1600957         but     1.529      24.40\n",
      "1600961          by     1.042      26.21\n",
      "1600985         can     1.421     208.24\n",
      "1601752          do     1.656     410.21\n",
      "1601758        does     5.299    6477.09\n",
      "1602246         for     4.686     376.65\n",
      "1602269     formula     1.565      46.16\n",
      "1602694          in     5.876     396.09\n",
      "1602887          is     2.611     148.20\n",
      "1602896          it     2.287     112.76\n",
      "1602901         its     1.818      94.24\n",
      "1603255         may     2.864    1352.24\n",
      "1603569         not     8.437   12938.41\n",
      "1603628          of    23.461   20132.64\n",
      "1603648          on    46.313  420371.01\n",
      "1603654        only     1.295     134.01\n",
      "1603678          or     2.485      85.61\n",
      "1603712       other     1.656      31.61\n",
      "1604155  properties     1.042      30.21\n",
      "1604541           s     2.161     125.85\n",
      "1604817        some     1.187      15.29\n",
      "1605014        such     1.439      27.45\n",
      "1605166        that     7.247    1492.61\n",
      "1605168         the    44.707   98586.04\n",
      "1605169       their     2.828     209.56\n",
      "1605193       these     1.944     180.01\n",
      "1605194        they     2.233     316.09\n",
      "1605200        this     1.908      71.09\n",
      "1605231          to     8.419    3941.16\n",
      "1605336        type     1.295     213.41\n",
      "1605408        upon     4.902    4984.01\n",
      "1605596       which     4.379     346.16\n",
      "1605620        will     3.784    2250.05\n",
      "1605661       would     1.601     412.44\n"
     ]
    }
   ],
   "source": [
    "filtered_by_C2 = C2_filter(tmp_df, filtered_by_C1, 'depend')\n",
    "### Print\n",
    "print(filtered_by_C2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blC4RZwwpWEE"
   },
   "source": [
    "<font color=\"green\">Expected output: </font> (The order isn't important.)\n",
    "\n",
    "> a {'strength': 6.381, 'spread': 777.29}   \n",
    "> all {'strength': 1.151, 'spread': 29.89}   \n",
    "> also {'strength': 1.133, 'spread': 208.96}   \n",
    "> an {'strength': 1.367, 'spread': 56.29}   \n",
    "> and {'strength': 15.183, 'spread': 2170.41}   \n",
    "> are {'strength': 1.962, 'spread': 98.84}   \n",
    "> as {'strength': 2.395, 'spread': 104.96}   \n",
    "> but {'strength': 1.529, 'spread': 24.4}   \n",
    "> by {'strength': 1.042, 'spread': 26.21}   \n",
    "> can {'strength': 1.421, 'spread': 208.24}   \n",
    "> do {'strength': 1.656, 'spread': 410.21}   \n",
    "> does {'strength': 5.299, 'spread': 6477.09}   \n",
    "> for {'strength': 4.686, 'spread': 376.65}   \n",
    "> formula {'strength': 1.565, 'spread': 46.16}   \n",
    "> in {'strength': 5.876, 'spread': 396.09}   \n",
    "> is {'strength': 2.611, 'spread': 148.2}   \n",
    "> it {'strength': 2.287, 'spread': 112.76}   \n",
    "> its {'strength': 1.818, 'spread': 94.24}   \n",
    "> may {'strength': 2.864, 'spread': 1352.24}   \n",
    "> not {'strength': 8.437, 'spread': 12938.41}   \n",
    "> of {'strength': 23.461, 'spread': 20132.64}   \n",
    "> on {'strength': 46.313, 'spread': 420371.01}   \n",
    "> only {'strength': 1.295, 'spread': 134.01}   \n",
    "> or {'strength': 2.485, 'spread': 85.61}   \n",
    "> other {'strength': 1.656, 'spread': 31.61}   \n",
    "> properties {'strength': 1.042, 'spread': 30.21}   \n",
    "> s {'strength': 2.161, 'spread': 125.85}   \n",
    "> some {'strength': 1.187, 'spread': 15.29}   \n",
    "> such {'strength': 1.439, 'spread': 27.45}   \n",
    "> that {'strength': 7.247, 'spread': 1492.61}   \n",
    "> the {'strength': 44.707, 'spread': 98586.04}   \n",
    "> their {'strength': 2.828, 'spread': 209.56}   \n",
    "> these {'strength': 1.944, 'spread': 180.01}   \n",
    "> they {'strength': 2.233, 'spread': 316.09}   \n",
    "> this {'strength': 1.908, 'spread': 71.09}   \n",
    "> to {'strength': 8.419, 'spread': 3941.16}   \n",
    "> type {'strength': 1.295, 'spread': 213.41}   \n",
    "> upon {'strength': 4.902, 'spread': 4984.01}   \n",
    "> which {'strength': 4.379, 'spread': 346.16}   \n",
    "> will {'strength': 3.784, 'spread': 2250.05}   \n",
    "> would {'strength': 1.601, 'spread': 412.44}   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "naS9EpYZpWEF"
   },
   "source": [
    "## C3 Condition\n",
    "C3 keeps the interesting collocates by pulling out the peaks of the $p^j_i$ distributions.\n",
    "\n",
    "Formula:\n",
    "\n",
    "$$p^j_i \\geq \\bar{p_i} + (k_1 \\times \\sqrt{U_{i}})$$\n",
    "\n",
    "where $U_i$ is *spread* in C2 and\n",
    "\n",
    "$k_1$ is equal to 1 \n",
    "\n",
    "<font color=\"red\">**[ TODO ]**</font> Please follow the condition to filter the result of last step and keep some which pass C3.\n",
    "\n",
    "The ouput sholud have `base word, collocate, distance, strength, spread, peak, count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "id": "1aHJ7GHlpWEF"
   },
   "outputs": [],
   "source": [
    "def C3_filter(base_word, filtered_by_C2, filter_word):\n",
    "    ### [TODO]\n",
    "    tmp = pd.DataFrame(columns=['W', 'Wi', 'Pi', 'strength', 'spread', 'peak', 'count'])\n",
    "    tmp1 = pd.DataFrame(columns=['W', 'Wi', 'Pi', 'strength', 'spread', 'peak', 'count'])\n",
    "    #[[''],[''],[0],[0],[0],[0],[0]], \n",
    "    bs = base_word[ base_word['W'] == filter_word ] #ÁØ©ÈÅ∏Âá∫depend \n",
    "    bs = bs[ bs['Wi'].isin(filtered_by_C2['Wi']) ] #ÁØ©ÈÅ∏Âá∫filterÁöÑWi\n",
    "    bs['Pi_bar'] = 0\n",
    "    for i in range(10):\n",
    "        bs['Pi_bar'] += bs[bs.columns[i+3]]\n",
    "    bs['Pi_bar'] = bs['Pi_bar']/10 #ÂæóÂà∞ÊØèÂàóÁöÑPi_bar\n",
    "    bs['spread'] = filtered_by_C2['spread'].astype(float) #ÂÖàÊääÂøÖË¶ÅÁöÑcoulumnÂä†ÈÄ≤‰æÜÔºåÊñπ‰æøÊìç‰Ωú\n",
    "    bs['strength'] = filtered_by_C2['strength'].astype(float) #ÂÖàÊääÂøÖË¶ÅÁöÑcoulumnÂä†ÈÄ≤‰æÜÔºåÊñπ‰æøÊìç‰Ωú\n",
    "    \n",
    "    for i in range(10): #‰æùÂ∫èË∑ë10ÂÄãdistance\n",
    "        bs_tmp = bs[ bs[bs.columns[i+3]] >= bs['Pi_bar'] + k1 * np.sqrt(bs['spread']) ] #ÁØ©ÈÅ∏C3\n",
    "        bs_tmp['Pi'] = int(bs_tmp.columns[i+3].replace(\"P\", \"\"))\n",
    "        bs_tmp['peak'] = (bs_tmp['Pi_bar'] + k1 * np.sqrt(bs_tmp['spread'])).round(3)\n",
    "        bs_tmp['count'] = bs_tmp[bs_tmp.columns[i+3]]\n",
    "        \n",
    "        tmp = bs_tmp[['W','Wi','Pi','strength','spread','peak', 'count']] #Áïô‰∏ãÈúÄË¶ÅÁöÑcolumn\n",
    "        frame = [tmp1, tmp] #ÊØèÂÅöÂ•Ω‰∏ÄÂÄãË∑ùÈõ¢Â∞±Âêà‰Ωµ\n",
    "        #print(tmp)\n",
    "        tmp1 = pd.concat(frame, ignore_index=True)\n",
    "    tmp1 = tmp1.sort_values(['Wi', 'Pi'], ascending=True)  #sortÊàêËàáÁµêÊûúÁõ∏Á¨¶   \n",
    "    return tmp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "id": "bFNox2TYpWEF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         W          Wi  Pi  strength     spread     peak count\n",
      "34  depend           a   2     6.381     777.29   63.780    94\n",
      "4   depend         all  -4     1.151      29.89   12.367    14\n",
      "10  depend         all  -3     1.151      29.89   12.367    16\n",
      "20  depend        also  -1     1.133     208.96   21.255    50\n",
      "35  depend          an   2     1.367      56.29   15.603    24\n",
      "53  depend          an   5     1.367      56.29   15.603    19\n",
      "44  depend         and   4    15.183    2170.41  131.288   149\n",
      "0   depend         are  -5     1.962      98.84   21.342    27\n",
      "5   depend         are  -4     1.962      98.84   21.342    22\n",
      "45  depend          as   4     2.395     104.96   24.045    30\n",
      "54  depend          as   5     2.395     104.96   24.045    28\n",
      "14  depend         but  -2     1.529      24.40   13.940    14\n",
      "55  depend         but   5     1.529      24.40   13.940    15\n",
      "1   depend          by  -5     1.042      26.21   11.420    13\n",
      "6   depend          by  -4     1.042      26.21   11.420    12\n",
      "46  depend          by   4     1.042      26.21   11.420    13\n",
      "21  depend         can  -1     1.421     208.24   22.831    49\n",
      "15  depend          do  -2     1.656     410.21   29.954    70\n",
      "16  depend        does  -2     5.299    6477.09  110.380   271\n",
      "47  depend         for   4     4.686     376.65   45.907    69\n",
      "7   depend     formula  -4     1.565      46.16   15.994    19\n",
      "36  depend     formula   2     1.565      46.16   15.994    17\n",
      "56  depend     formula   5     1.565      46.16   15.994    19\n",
      "2   depend          in  -5     5.876     396.09   53.002    55\n",
      "48  depend          in   4     5.876     396.09   53.002    62\n",
      "3   depend          is  -5     2.611     148.20   27.174    37\n",
      "57  depend          is   5     2.611     148.20   27.174    29\n",
      "11  depend          it  -3     2.287     112.76   23.819    39\n",
      "17  depend          it  -2     2.287     112.76   23.819    24\n",
      "37  depend         its   2     1.818      94.24   20.308    36\n",
      "22  depend         may  -1     2.864    1352.24   53.173   126\n",
      "23  depend         not  -1     8.437   12938.41  161.047   388\n",
      "49  depend          of   4    23.461   20132.64  272.490   495\n",
      "31  depend          on   1    46.313  420371.01  905.660  2195\n",
      "32  depend        only   1     1.295     134.01   19.276    40\n",
      "50  depend          or   4     2.485      85.61   23.553    29\n",
      "58  depend          or   5     2.485      85.61   23.553    25\n",
      "41  depend       other   3     1.656      31.61   15.322    19\n",
      "59  depend       other   5     1.656      31.61   15.322    17\n",
      "8   depend  properties  -4     1.042      30.21   11.796    12\n",
      "24  depend  properties  -1     1.042      30.21   11.796    15\n",
      "42  depend  properties   3     1.042      30.21   11.796    15\n",
      "51  depend           s   4     2.161     125.85   23.718    41\n",
      "12  depend        some  -3     1.187      15.29   11.010    13\n",
      "38  depend        some   2     1.187      15.29   11.010    14\n",
      "52  depend        such   4     1.439      27.45   13.739    17\n",
      "13  depend        that  -3     7.247    1492.61   79.334    84\n",
      "25  depend        that  -1     7.247    1492.61   79.334   132\n",
      "39  depend         the   2    44.707   98586.04  562.384  1140\n",
      "40  depend       their   2     2.828     209.56   30.676    52\n",
      "18  depend       these  -2     1.944     180.01   24.717    48\n",
      "26  depend        they  -1     2.233     316.09   30.679    63\n",
      "9   depend        this  -4     1.908      71.09   19.531    28\n",
      "19  depend        this  -2     1.908      71.09   19.531    22\n",
      "27  depend          to  -1     8.419    3941.16  109.979   228\n",
      "43  depend        type   3     1.295     213.41   22.309    50\n",
      "33  depend        upon   1     4.902    4984.01   98.298   239\n",
      "28  depend       which  -1     4.379     346.16   43.405    66\n",
      "29  depend        will  -1     3.784    2250.05   68.935   159\n",
      "30  depend       would  -1     1.601     412.44   29.709    70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a0985\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\a0985\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\a0985\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "filtered_by_C3 = C3_filter(tmp_df, filtered_by_C2, 'depend')\n",
    "### Print\n",
    "print(filtered_by_C3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9K43nWOqpWEF"
   },
   "source": [
    "<font color=\"green\">Expected output: </font> (The order isn't important.)\n",
    "\n",
    "> ('depend', 'a', 2) {'strength': 6.381, 'spread': 777.29, 'peak': 63.78, 'count': 94}   \n",
    "> ('depend', 'all', -4) {'strength': 1.151, 'spread': 29.89, 'peak': 12.367, 'count': 14}   \n",
    "> ('depend', 'all', -3) {'strength': 1.151, 'spread': 29.89, 'peak': 12.367, 'count': 16}   \n",
    "> ('depend', 'also', -1) {'strength': 1.133, 'spread': 208.96, 'peak': 21.255, 'count': 50}   \n",
    "> ('depend', 'an', 2) {'strength': 1.367, 'spread': 56.29, 'peak': 15.603, 'count': 24}   \n",
    "> ('depend', 'an', 5) {'strength': 1.367, 'spread': 56.29, 'peak': 15.603, 'count': 19}   \n",
    "> ('depend', 'and', 4) {'strength': 15.183, 'spread': 2170.41, 'peak': 131.288, 'count': 149}   \n",
    "> ('depend', 'are', -5) {'strength': 1.962, 'spread': 98.84, 'peak': 21.342, 'count': 27}   \n",
    "> ('depend', 'are', -4) {'strength': 1.962, 'spread': 98.84, 'peak': 21.342, 'count': 22}   \n",
    "> ('depend', 'as', 4) {'strength': 2.395, 'spread': 104.96, 'peak': 24.045, 'count': 30}   \n",
    "> ('depend', 'as', 5) {'strength': 2.395, 'spread': 104.96, 'peak': 24.045, 'count': 28}   \n",
    "> ('depend', 'but', -2) {'strength': 1.529, 'spread': 24.4, 'peak': 13.94, 'count': 14}   \n",
    "> ('depend', 'but', 5) {'strength': 1.529, 'spread': 24.4, 'peak': 13.94, 'count': 15}   \n",
    "> ('depend', 'by', -5) {'strength': 1.042, 'spread': 26.21, 'peak': 11.42, 'count': 13}   \n",
    "> ('depend', 'by', -4) {'strength': 1.042, 'spread': 26.21, 'peak': 11.42, 'count': 12}   \n",
    "> ('depend', 'by', 4) {'strength': 1.042, 'spread': 26.21, 'peak': 11.42, 'count': 13}   \n",
    "> ('depend', 'can', -1) {'strength': 1.421, 'spread': 208.24, 'peak': 22.831, 'count': 49}   \n",
    "> ('depend', 'do', -2) {'strength': 1.656, 'spread': 410.21, 'peak': 29.954, 'count': 70}   \n",
    "> ('depend', 'does', -2) {'strength': 5.299, 'spread': 6477.09, 'peak': 110.38, 'count': 271}   \n",
    "> ('depend', 'for', 4) {'strength': 4.686, 'spread': 376.65, 'peak': 45.907, 'count': 69}   \n",
    "> ('depend', 'formula', -4) {'strength': 1.565, 'spread': 46.16, 'peak': 15.994, 'count': 19}   \n",
    "> ('depend', 'formula', 2) {'strength': 1.565, 'spread': 46.16, 'peak': 15.994, 'count': 17}   \n",
    "> ('depend', 'formula', 5) {'strength': 1.565, 'spread': 46.16, 'peak': 15.994, 'count': 19}   \n",
    "> ('depend', 'in', -5) {'strength': 5.876, 'spread': 396.09, 'peak': 53.002, 'count': 55}   \n",
    "> ('depend', 'in', 4) {'strength': 5.876, 'spread': 396.09, 'peak': 53.002, 'count': 62}   \n",
    "> ('depend', 'is', -5) {'strength': 2.611, 'spread': 148.2, 'peak': 27.174, 'count': 37}   \n",
    "> ('depend', 'is', 5) {'strength': 2.611, 'spread': 148.2, 'peak': 27.174, 'count': 29}   \n",
    "> ('depend', 'it', -3) {'strength': 2.287, 'spread': 112.76, 'peak': 23.819, 'count': 39}   \n",
    "> ('depend', 'it', -2) {'strength': 2.287, 'spread': 112.76, 'peak': 23.819, 'count': 24}   \n",
    "> ('depend', 'its', 2) {'strength': 1.818, 'spread': 94.24, 'peak': 20.308, 'count': 36}   \n",
    "> ('depend', 'may', -1) {'strength': 2.864, 'spread': 1352.24, 'peak': 53.173, 'count': 126}   \n",
    "> ('depend', 'not', -1) {'strength': 8.437, 'spread': 12938.41, 'peak': 161.047, 'count': 388}   \n",
    "> ('depend', 'of', 4) {'strength': 23.461, 'spread': 20132.64, 'peak': 272.49, 'count': 495}   \n",
    "> ('depend', 'on', 1) {'strength': 46.313, 'spread': 420371.01, 'peak': 905.66, 'count': 2195}   \n",
    "> ('depend', 'only', 1) {'strength': 1.295, 'spread': 134.01, 'peak': 19.276, 'count': 40}   \n",
    "> ('depend', 'or', 4) {'strength': 2.485, 'spread': 85.61, 'peak': 23.553, 'count': 29}   \n",
    "> ('depend', 'or', 5) {'strength': 2.485, 'spread': 85.61, 'peak': 23.553, 'count': 25}   \n",
    "> ('depend', 'other', 3) {'strength': 1.656, 'spread': 31.61, 'peak': 15.322, 'count': 19}   \n",
    "> ('depend', 'other', 5) {'strength': 1.656, 'spread': 31.61, 'peak': 15.322, 'count': 17}   \n",
    "> ('depend', 'properties', -4) {'strength': 1.042, 'spread': 30.21, 'peak': 11.796, 'count': 12}   \n",
    "> ('depend', 'properties', -1) {'strength': 1.042, 'spread': 30.21, 'peak': 11.796, 'count': 15}   \n",
    "> ('depend', 'properties', 3) {'strength': 1.042, 'spread': 30.21, 'peak': 11.796, 'count': 15}   \n",
    "> ('depend', 's', 4) {'strength': 2.161, 'spread': 125.85, 'peak': 23.718, 'count': 41}   \n",
    "> ('depend', 'some', -3) {'strength': 1.187, 'spread': 15.29, 'peak': 11.01, 'count': 13}   \n",
    "> ('depend', 'some', 2) {'strength': 1.187, 'spread': 15.29, 'peak': 11.01, 'count': 14}   \n",
    "> ('depend', 'such', 4) {'strength': 1.439, 'spread': 27.45, 'peak': 13.739, 'count': 17}   \n",
    "> ('depend', 'that', -3) {'strength': 7.247, 'spread': 1492.61, 'peak': 79.334, 'count': 84}   \n",
    "> ('depend', 'that', -1) {'strength': 7.247, 'spread': 1492.61, 'peak': 79.334, 'count': 132}   \n",
    "> ('depend', 'the', 2) {'strength': 44.707, 'spread': 98586.04, 'peak': 562.384, 'count': 1140}   \n",
    "> ('depend', 'their', 2) {'strength': 2.828, 'spread': 209.56, 'peak': 30.676, 'count': 52}   \n",
    "> ('depend', 'these', -2) {'strength': 1.944, 'spread': 180.01, 'peak': 24.717, 'count': 48}   \n",
    "> ('depend', 'they', -1) {'strength': 2.233, 'spread': 316.09, 'peak': 30.679, 'count': 63}   \n",
    "> ('depend', 'this', -4) {'strength': 1.908, 'spread': 71.09, 'peak': 19.531, 'count': 28}   \n",
    "> ('depend', 'this', -2) {'strength': 1.908, 'spread': 71.09, 'peak': 19.531, 'count': 22}   \n",
    "> ('depend', 'to', -1) {'strength': 8.419, 'spread': 3941.16, 'peak': 109.979, 'count': 228}   \n",
    "> ('depend', 'type', 3) {'strength': 1.295, 'spread': 213.41, 'peak': 22.309, 'count': 50}   \n",
    "> ('depend', 'upon', 1) {'strength': 4.902, 'spread': 4984.01, 'peak': 98.298, 'count': 239}   \n",
    "> ('depend', 'which', -1) {'strength': 4.379, 'spread': 346.16, 'peak': 43.405, 'count': 66}   \n",
    "> ('depend', 'will', -1) {'strength': 3.784, 'spread': 2250.05, 'peak': 68.935, 'count': 159}   \n",
    "> ('depend', 'would', -1) {'strength': 1.601, 'spread': 412.44, 'peak': 29.709, 'count': 70}   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3bMqATOZpWEG"
   },
   "source": [
    "## Strongest Collocation\n",
    "There are too many collocations to check your result easily. Hence, we want you use the rules below to find out one strongest collocation for \"depend\".\n",
    "\n",
    "Rule:\n",
    "1. find the collocate with maximum **`strength`** value\n",
    "2. find the collocate with maximum **`count`** value\n",
    "\n",
    "If there're more than two collocations sharing same maximum `strength` value, please use rule 2 to find one as the answer. Otherwise, you can ignore Rule 2.\n",
    "\n",
    "<font color=\"red\">**[ TODO ]**</font> Please find out the strongest collocation for \"depend\" by the rules.\n",
    "\n",
    "The ouput format sholud be `(base word, collocate, distance)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "id": "idhTENjepWEG"
   },
   "outputs": [],
   "source": [
    "def find_strongest_collocation(base_word, filtered_by_C3):\n",
    "    ### [TODO]\n",
    "    tmp = filtered_by_C3[filtered_by_C3['strength']==filtered_by_C3['strength'].max()]\n",
    "    if tmp.shape[0] > 1: #Â¶ÇÊûúÁ¨¨‰∏ÄÂ±§ÁØ©ÈÅ∏ÊúâÂ§öÂÄãrowÂâáÁî®rule 2ÂÅöÊúÄÁµÇÁØ©ÈÅ∏\n",
    "        tmp = tmp[tmp['count']==tmp['count'].max()]\n",
    "    result = (tmp['W'].values[0], tmp['Wi'].values[0], tmp['Pi'].values[0])\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "id": "dPxhbWqYpWEG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('depend', 'on', 1)\n"
     ]
    }
   ],
   "source": [
    "find_strongest_collocation(tmp_df, filtered_by_C3)\n",
    "### Run and Print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQFst4LLpWEG"
   },
   "source": [
    "<font color=\"green\">Expected output: </font>\n",
    "\n",
    "> ('depend', 'on', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ub8AdaebpWEG"
   },
   "source": [
    "## Find Helpful AKL Collocation\n",
    "Only one example cannot express how amazing what we just did, so here are some other AKL verbs selected for you to experience. \n",
    "\n",
    "<font color=\"red\">**[ TODO ]**</font> Please finish **combination** function to combine last four functions together and use it to find out strongest collocations for **AKL_verbs**. \n",
    "\n",
    "The ouput format sholud be `(base word, collocate, distance)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "id": "fPZ6ulIJpWEG"
   },
   "outputs": [],
   "source": [
    "AKL_verbs = ['argue', 'can', 'consist', 'contrast', 'favour', 'lack', 'may', \n",
    "            'neglect', 'participate', 'present', 'rely', 'suggest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "id": "Yolxm1hfpWEG"
   },
   "outputs": [],
   "source": [
    "def combination(base_word, AKL_verbs):\n",
    "    ### [TODO]\n",
    "    for i in AKL_verbs:\n",
    "        filtered_by_C1 = C1_filter(base_word, i)\n",
    "        filtered_by_C2 = C2_filter(base_word, filtered_by_C1, i)\n",
    "        filtered_by_C3 = C3_filter(base_word, filtered_by_C2, i)\n",
    "        find_strongest_collocation(base_word, filtered_by_C3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "id": "31okIDBMpWEG"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a0985\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\a0985\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\a0985\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\a0985\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('argue', 'that', 1)\n",
      "('can', 'be', 1)\n",
      "('consist', 'of', 1)\n",
      "('contrast', 'in', -1)\n",
      "('favour', 'of', 1)\n",
      "('lack', 'of', 1)\n",
      "('may', 'be', 1)\n",
      "('neglect', 'of', 1)\n",
      "('participate', 'in', 1)\n",
      "('present', 'with', -3)\n",
      "('rely', 'on', 1)\n",
      "('suggest', 'that', 1)\n"
     ]
    }
   ],
   "source": [
    "### Run and Print\n",
    "combination(tmp_df, AKL_verbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-laz0HgapWEH"
   },
   "source": [
    "<font color=\"green\">Expected output: </font>\n",
    "\n",
    "> ('argue', 'that', 1)   \n",
    "> ('can', 'be', 1)   \n",
    "> ('consist', 'of', 1)   \n",
    "> ('contrast', 'in', -1)   \n",
    "> ('favour', 'of', 1)   \n",
    "> ('lack', 'of', 1)   \n",
    "> ('may', 'be', 1)   \n",
    "> ('neglect', 'of', 1)   \n",
    "> ('participate', 'in', 1)   \n",
    "> ('present', 'with', -3)   \n",
    "> ('rely', 'on', 1)   \n",
    "> ('suggest', 'that', 1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XO64hc-OpWEH"
   },
   "source": [
    "## TA's Notes\n",
    "\n",
    "If you complete the Assignment, please use [this link](https://docs.google.com/spreadsheets/d/1QGeYl5dsD9sFO9SYg4DIKk-xr-yGjRDOOLKZqCLDv2E/edit#gid=206119035) to reserve demo time.  \n",
    "The score is only given after TAs review your implementation, so <u>**make sure you make a appointment with a TA before you miss the deadline**</u> .  <br>After demo, please upload your assignment to eeclass. You just need to hand in this ipynb file and rename it as XXXXXXXXX(Your student ID).ipynb.\n",
    "<br>Note that **late submission will not be allowed**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oayUKgdNpWEH"
   },
   "source": [
    "## Reference\n",
    "[Frank Smadja, Retrieving Collocations from Texts: Xtract, Computational Linguistics, Volume 19, 1993](https://aclanthology.org/J93-1007.pdf)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Assignment6.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
